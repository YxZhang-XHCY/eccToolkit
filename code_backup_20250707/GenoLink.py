import pandas as pd
from itertools import combinations
import random
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import numpy as np
from pathlib import Path
import argparse
import logging
from typing import Tuple, List, Dict
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# é¢„ç”Ÿæˆé¢œè‰²æ± ï¼Œé¿å…é‡å¤è®¡ç®—
COLOR_POOL = [(r, g, b) for r in range(0, 256, 15) for g in range(0, 256, 15) for b in range(0, 256, 15)]

def generate_random_rgb() -> str:
    """ç”ŸæˆéšæœºRGBé¢œè‰²å­—ç¬¦ä¸²"""
    if COLOR_POOL:
        r, g, b = random.choice(COLOR_POOL)
    else:
        r, g, b = random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)
    return f"{r},{g},{b}"

def generate_distinct_color(used_colors: set) -> str:
    """ç”Ÿæˆä¸å·²ä½¿ç”¨é¢œè‰²æœ‰è¶³å¤ŸåŒºåˆ†åº¦çš„æ–°é¢œè‰²"""
    max_attempts = 100
    min_distance = 50  # æœ€å°é¢œè‰²è·ç¦»
    
    for _ in range(max_attempts):
        new_color = generate_random_rgb()
        r1, g1, b1 = map(int, new_color.split(','))
        
        is_distinct = True
        for used in used_colors:
            r2, g2, b2 = map(int, used.split(','))
            # è®¡ç®—æ¬§æ°è·ç¦»
            distance = ((r1-r2)**2 + (g1-g2)**2 + (b1-b2)**2) ** 0.5
            if distance < min_distance:
                is_distinct = False
                break
        
        if is_distinct:
            return new_color
    
    # å¦‚æœæ‰¾ä¸åˆ°è¶³å¤Ÿä¸åŒçš„é¢œè‰²ï¼Œè¿”å›éšæœºé¢œè‰²
    return generate_random_rgb()

def validate_input_data(df: pd.DataFrame) -> bool:
    """éªŒè¯è¾“å…¥æ•°æ®çš„å®Œæ•´æ€§"""
    required_columns = ['eName', 'eChr', 'eStart', 'eEnd']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.error(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
        return False
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    if not pd.api.types.is_numeric_dtype(df['eStart']) or not pd.api.types.is_numeric_dtype(df['eEnd']):
        logger.error("eStartå’ŒeEndå¿…é¡»æ˜¯æ•°å€¼ç±»å‹")
        return False
    
    # æ£€æŸ¥åæ ‡åˆç†æ€§
    invalid_coords = df[df['eStart'] >= df['eEnd']]
    if not invalid_coords.empty:
        logger.warning(f"å‘ç°{len(invalid_coords)}è¡Œåæ ‡å¼‚å¸¸(eStart >= eEnd)")
    
    return True

def process_group(args: Tuple[str, pd.DataFrame, bool]) -> List[List]:
    """å¤„ç†æ¯ä¸ªåˆ†ç»„çš„æ•°æ®"""
    name, group_df, use_distinct_colors = args
    
    # è·³è¿‡åªæœ‰ä¸€ä¸ªå…ƒç´ çš„ç»„
    if len(group_df) < 2:
        return []
    
    # ç”Ÿæˆé¢œè‰²
    if use_distinct_colors:
        # ä½¿ç”¨é™æ€é¢œè‰²é›†åˆï¼ˆåœ¨ä¸»è¿›ç¨‹ä¸­é¢„ç”Ÿæˆï¼‰
        color = generate_random_rgb()
    else:
        color = generate_random_rgb()
    
    output = []
    # ä½¿ç”¨numpyæ•°ç»„æ“ä½œæé«˜æ•ˆç‡
    data_array = group_df[['eChr', 'eStart', 'eEnd']].values
    
    for i in range(len(data_array)):
        for j in range(i + 1, len(data_array)):
            output.append([
                data_array[i][0], int(data_array[i][1]), int(data_array[i][2]),
                data_array[j][0], int(data_array[j][1]), int(data_array[j][2]),
                color
            ])
    
    return output

def process_large_file_by_group(input_file: str, output_file: str, 
                               use_distinct_colors: bool = False, n_processes: int = None,
                               batch_size: int = 1000):
    """å¤„ç†å¤§æ–‡ä»¶çš„å‡½æ•°ï¼ŒæŒ‰ç»„æ‰¹é‡å¤„ç†ä»¥ä¿æŒç»„çš„å®Œæ•´æ€§"""
    if n_processes is None:
        n_processes = cpu_count()
    
    # å…ˆè¯»å–æ–‡ä»¶å¤´ä»¥éªŒè¯æ ¼å¼
    df_head = pd.read_csv(input_file, nrows=10)
    if not validate_input_data(df_head):
        raise ValueError("è¾“å…¥æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
    
    logger.info("ç¬¬ä¸€æ­¥ï¼šæ‰«ææ–‡ä»¶ï¼Œç»Ÿè®¡æ¯ä¸ªeNameçš„è¡Œæ•°...")
    
    # ç¬¬ä¸€éæ‰«æï¼šç»Ÿè®¡æ¯ä¸ªeNameçš„å¤§å°
    ename_counts = {}
    total_rows = 0
    
    with open(input_file, 'r') as f:
        header = f.readline()
        header_parts = header.strip().split(',')
        ename_idx = header_parts.index('eName')
        
        for line in tqdm(f, desc="ç»Ÿè®¡eNameåˆ†å¸ƒ"):
            total_rows += 1
            parts = line.strip().split(',')
            if len(parts) > ename_idx:
                ename = parts[ename_idx]
                ename_counts[ename] = ename_counts.get(ename, 0) + 1
    
    logger.info(f"æ–‡ä»¶æ€»è¡Œæ•°: {total_rows}")
    logger.info(f"å”¯ä¸€eNameæ•°: {len(ename_counts)}")
    
    # æŒ‰æ‰¹æ¬¡å¤„ç†eName
    enames = list(ename_counts.keys())
    valid_groups = [name for name in enames if ename_counts[name] > 1]
    logger.info(f"æœ‰æ•ˆåˆ†ç»„æ•°(å…ƒç´ >1): {len(valid_groups)}")
    
    if not valid_groups:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„åˆ†ç»„æ•°æ®")
        pd.DataFrame(columns=[
            "Chr1", "Start1", "End1", "Chr2", "Start2", "End2", "RGB"
        ]).to_csv(output_file, sep="\t", index=False)
        return
    
    # åˆ†æ‰¹å¤„ç†
    all_results = []
    
    for i in range(0, len(valid_groups), batch_size):
        batch_enames = set(valid_groups[i:i+batch_size])
        logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(valid_groups)-1)//batch_size + 1}")
        
        # è¯»å–è¿™æ‰¹eNameçš„æ‰€æœ‰æ•°æ®
        batch_df = pd.read_csv(input_file)
        batch_df = batch_df[batch_df['eName'].isin(batch_enames)]
        
        if batch_df.empty:
            continue
        
        # åˆ†ç»„å¤„ç†
        groups = list(batch_df.groupby("eName"))
        process_args = [(name, group_df, use_distinct_colors) for name, group_df in groups]
        
        # å¹¶è¡Œå¤„ç†
        with Pool(processes=n_processes) as pool:
            batch_results = list(tqdm(
                pool.imap_unordered(process_group, process_args), 
                total=len(groups),
                desc=f"Processing batch {i//batch_size + 1}"
            ))
        
        # æ”¶é›†ç»“æœ
        for group_result in batch_results:
            all_results.extend(group_result)
    
    # å†™å…¥ç»“æœ
    output_df = pd.DataFrame(all_results, columns=[
        "Chr1", "Start1", "End1", "Chr2", "Start2", "End2", "RGB"
    ])
    output_df.to_csv(output_file, sep="\t", index=False)
    logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(output_df)} æ¡é“¾æ¥")

def process_large_file_streaming(input_file: str, output_file: str,
                                use_distinct_colors: bool = False, n_processes: int = None):
    """ä½¿ç”¨æµå¼å¤„ç†å¤§æ–‡ä»¶ï¼Œå°†å®Œæ•´çš„ç»„å†™å…¥ä¸´æ—¶æ–‡ä»¶åæ‰¹é‡å¤„ç†"""
    import tempfile
    import shutil
    
    if n_processes is None:
        n_processes = cpu_count()
    
    # å…ˆè¯»å–æ–‡ä»¶å¤´
    with open(input_file, 'r') as f:
        header = f.readline().strip()
    
    df_head = pd.read_csv(input_file, nrows=10)
    if not validate_input_data(df_head):
        raise ValueError("è¾“å…¥æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
    
    logger.info("ä½¿ç”¨æµå¼å¤„ç†æ¨¡å¼...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # ç¬¬ä¸€æ­¥ï¼šæŒ‰eNameåˆ†å‰²æ–‡ä»¶
        logger.info("ç¬¬ä¸€æ­¥ï¼šæŒ‰eNameåˆ†å‰²æ•°æ®...")
        ename_files = {}
        ename_counts = {}
        
        with open(input_file, 'r') as f:
            header_line = f.readline()
            header_parts = header_line.strip().split(',')
            ename_idx = header_parts.index('eName')
            
            for line in tqdm(f, desc="åˆ†å‰²æ•°æ®"):
                parts = line.strip().split(',')
                if len(parts) > ename_idx:
                    ename = parts[ename_idx]
                    
                    if ename not in ename_files:
                        temp_file = temp_path / f"{ename}.csv"
                        ename_files[ename] = open(temp_file, 'w')
                        ename_files[ename].write(header_line)
                        ename_counts[ename] = 0
                    
                    ename_files[ename].write(line)
                    ename_counts[ename] += 1
        
        # å…³é—­æ‰€æœ‰æ–‡ä»¶
        for f in ename_files.values():
            f.close()
        
        # ç­›é€‰æœ‰æ•ˆç»„
        valid_enames = [name for name, count in ename_counts.items() if count > 1]
        logger.info(f"æœ‰æ•ˆåˆ†ç»„æ•°: {len(valid_enames)}")
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡å¤„ç†æ¯ä¸ªç»„
        logger.info("ç¬¬äºŒæ­¥ï¼šå¤„ç†æ¯ä¸ªåˆ†ç»„...")
        all_results = []
        
        # å‡†å¤‡å¤„ç†å‚æ•°
        process_args = []
        for ename in valid_enames:
            temp_file = temp_path / f"{ename}.csv"
            if temp_file.exists():
                group_df = pd.read_csv(temp_file)
                process_args.append((ename, group_df, use_distinct_colors))
        
        # å¹¶è¡Œå¤„ç†
        with Pool(processes=n_processes) as pool:
            results = list(tqdm(
                pool.imap_unordered(process_group, process_args),
                total=len(process_args),
                desc="Processing groups"
            ))
        
        # æ”¶é›†ç»“æœ
        for group_result in results:
            all_results.extend(group_result)
    
    # å†™å…¥æœ€ç»ˆç»“æœ
    output_df = pd.DataFrame(all_results, columns=[
        "Chr1", "Start1", "End1", "Chr2", "Start2", "End2", "RGB"
    ])
    output_df.to_csv(output_file, sep="\t", index=False)
    logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(output_df)} æ¡é“¾æ¥")

def main():
    parser = argparse.ArgumentParser(description='ç”ŸæˆåŸºå› ç»„é“¾æ¥å…³ç³»å›¾æ•°æ®')
    parser.add_argument('input', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºTSVæ–‡ä»¶è·¯å¾„', default=None)
    parser.add_argument('-p', '--processes', type=int, default=None,
                       help='ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°ï¼‰')
    parser.add_argument('--distinct-colors', action='store_true',
                       help='å°è¯•ç”Ÿæˆæ›´æœ‰åŒºåˆ†åº¦çš„é¢œè‰²')
    parser.add_argument('--seed', type=int, default=None,
                       help='éšæœºç§å­ï¼ˆç”¨äºå¯é‡å¤çš„é¢œè‰²ç”Ÿæˆï¼‰')
    parser.add_argument('--mode', choices=['auto', 'memory', 'streaming'], default='auto',
                       help='å¤„ç†æ¨¡å¼ï¼šauto(è‡ªåŠ¨é€‰æ‹©), memory(å…¨éƒ¨è½½å…¥å†…å­˜), streaming(æµå¼å¤„ç†)')
    parser.add_argument('--batch-size', type=int, default=1000,
                       help='æ‰¹å¤„ç†å¤§å°ï¼ˆç”¨äºå¤§æ–‡ä»¶å¤„ç†ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        random.seed(args.seed)
        np.random.seed(args.seed)
    
    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if args.output is None:
        input_path = Path(args.input)
        args.output = str(input_path.with_suffix('.Links.RandomColor.tsv'))
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.input).exists():
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)
    
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = Path(args.input).stat().st_size / (1024 * 1024)  # MB
        logger.info(f"è¾“å…¥æ–‡ä»¶å¤§å°: {file_size:.1f}MB")
        
        # å†³å®šå¤„ç†æ¨¡å¼
        if args.mode == 'auto':
            if file_size > 500:  # å¤§äº500MBä½¿ç”¨æµå¼å¤„ç†
                mode = 'streaming'
            elif file_size > 100:  # 100-500MBä½¿ç”¨æ‰¹å¤„ç†
                mode = 'batch'
            else:
                mode = 'memory'
        else:
            mode = args.mode
        
        if mode == 'streaming':
            logger.info("ä½¿ç”¨æµå¼å¤„ç†æ¨¡å¼ï¼ˆé€‚åˆè¶…å¤§æ–‡ä»¶ï¼‰")
            process_large_file_streaming(args.input, args.output, 
                                       args.distinct_colors, args.processes)
        elif mode == 'batch' or (mode == 'memory' and file_size > 100):
            logger.info("ä½¿ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼")
            process_large_file_by_group(args.input, args.output, 
                                      args.distinct_colors, args.processes,
                                      args.batch_size)
        else:
            # å°æ–‡ä»¶ç›´æ¥å¤„ç†
            logger.info("ä½¿ç”¨å†…å­˜å¤„ç†æ¨¡å¼")
            df = pd.read_csv(args.input)
            
            # éªŒè¯æ•°æ®
            if not validate_input_data(df):
                sys.exit(1)
            
            # ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"æ•°æ®ç»Ÿè®¡:")
            logger.info(f"  - æ€»è¡Œæ•°: {len(df)}")
            logger.info(f"  - å”¯ä¸€eNameæ•°: {df['eName'].nunique()}")
            logger.info(f"  - æŸ“è‰²ä½“: {sorted(df['eChr'].unique())}")
            
            # åˆ†ç»„
            groups = list(df.groupby("eName"))
            logger.info(f"ğŸ“Š æ€»åˆ†ç»„æ•°: {len(groups)} | CPUæ ¸å¿ƒæ•°: {cpu_count()}")
            
            # è¿‡æ»¤æ‰åªæœ‰ä¸€ä¸ªå…ƒç´ çš„ç»„
            groups_filtered = [(name, group) for name, group in groups if len(group) > 1]
            logger.info(f"æœ‰æ•ˆåˆ†ç»„æ•°(å…ƒç´ >1): {len(groups_filtered)}")
            
            if not groups_filtered:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„åˆ†ç»„æ•°æ®ï¼ˆæ‰€æœ‰ç»„éƒ½åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼‰")
                # åˆ›å»ºç©ºçš„è¾“å‡ºæ–‡ä»¶
                pd.DataFrame(columns=[
                    "Chr1", "Start1", "End1", "Chr2", "Start2", "End2", "RGB"
                ]).to_csv(args.output, sep="\t", index=False)
                return
            
            # å‡†å¤‡å‚æ•°
            process_args = [(name, group_df, args.distinct_colors) 
                          for name, group_df in groups_filtered]
            
            # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
            n_processes = args.processes or cpu_count()
            with Pool(processes=n_processes) as pool:
                results = list(tqdm(
                    pool.imap_unordered(process_group, process_args), 
                    total=len(groups_filtered),
                    desc="Processing groups"
                ))
            
            # å±•å¹³ç»“æœ
            flat_results = [row for group_result in results for row in group_result]
            
            # åˆ›å»ºè¾“å‡ºDataFrame
            output_df = pd.DataFrame(flat_results, columns=[
                "Chr1", "Start1", "End1", "Chr2", "Start2", "End2", "RGB"
            ])
            
            # ç»Ÿè®¡è¾“å‡ºä¿¡æ¯
            logger.info(f"è¾“å‡ºç»Ÿè®¡:")
            logger.info(f"  - æ€»é“¾æ¥æ•°: {len(output_df)}")
            logger.info(f"  - å”¯ä¸€é¢œè‰²æ•°: {output_df['RGB'].nunique()}")
            
            # ä¿å­˜ç»“æœ
            output_df.to_csv(args.output, sep="\t", index=False)
            logger.info(f"âœ… å®Œæˆï¼è¾“å‡ºä¿å­˜åˆ°: {args.output}")
            
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise

if __name__ == "__main__":
    main()
